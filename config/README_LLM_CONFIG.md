# Configuration des LLM Providers - Guide d'utilisation

## üéØ Architecture de Configuration

La configuration des LLM suit une **architecture en deux niveaux** :

### Niveau 1 : Configuration Transversale (global.yaml)

**Localisation :** `config/global.yaml` ‚Üí section `llm_providers`

**R√¥le :** D√©finir les **connexions** aux services LLM (infrastructure)

**Contient :**
- `base_url` : URL du service API
- `api_key` : Cl√© d'authentification (ou `${ENV_VAR}`)
- `access_method` : M√©thode d'acc√®s (openai_compatible, huggingface_inference_api)

### Niveau 2 : Configuration Fonctionnelle (par √©tape)

**Localisation :** Chaque fichier `config/XX_step_name.yaml`

**R√¥le :** Choisir **quel mod√®le utiliser** pour cette t√¢che sp√©cifique

**Contient :**
- `provider` : Nom du provider (r√©f√©rence √† global.yaml)
- `model` : Mod√®le sp√©cifique (ex: "llama3", "mistral-large-latest")
- `temperature` : Temp√©rature pour cette t√¢che (0.0 = d√©terministe, 1.0 = cr√©atif)
- `max_tokens` : Limite de tokens pour les r√©ponses

## üìã Providers Disponibles

### Providers Locaux (Gratuits)

#### 1. LM Studio
```yaml
# global.yaml
llm_providers:
  lm_studio:
    access_method: "openai_compatible"
    base_url: "http://127.0.0.1:1234/v1"
    api_key: "lm-studio"
```

**Utilisation dans une √©tape :**
```yaml
# 04_enrichment.yaml
llm:
  enabled: true
  provider: "lm_studio"
  model: "llama-3.1-8b-instruct"  # Nom du mod√®le charg√© dans LM Studio
  temperature: 0.0
```

**Installation :** https://lmstudio.ai/

#### 2. Ollama
```yaml
# global.yaml
llm_providers:
  ollama:
    access_method: "openai_compatible"
    base_url: "http://127.0.0.1:11434/v1"
    api_key: "ollama"
```

**Utilisation dans une √©tape :**
```yaml
# 04_enrichment.yaml
llm:
  enabled: true
  provider: "ollama"
  model: "llama3"  # ou "mistral", "gemma2", etc.
  temperature: 0.0
```

**Installation :**
```bash
# macOS
brew install ollama
ollama serve

# T√©l√©charger un mod√®le
ollama pull llama3
```

#### 3. vLLM (Production)
```yaml
# global.yaml
llm_providers:
  vllm:
    access_method: "openai_compatible"
    base_url: "http://127.0.0.1:8000/v1"
    api_key: "vllm"
```

**Utilisation :**
```bash
# D√©marrer vLLM
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-3-8B-Instruct \
    --port 8000
```

### Providers Cloud (Payants)

#### 4. Hugging Face
```yaml
# global.yaml
llm_providers:
  huggingface:
    access_method: "huggingface_inference_api"
    base_url: "https://api-inference.huggingface.co/v1"
    api_key: "${HUGGINGFACE_API_KEY}"
```

**Configuration de la cl√© :**
```bash
export HUGGINGFACE_API_KEY="hf_your_actual_key_here"
```

**Utilisation dans une √©tape :**
```yaml
llm:
  enabled: true
  provider: "huggingface"
  model: "mistralai/Mistral-7B-Instruct-v0.2"
  temperature: 0.0
```

#### 5. Mistral AI
```yaml
# global.yaml
llm_providers:
  mistral_ai:
    access_method: "openai_compatible"
    base_url: "https://api.mistral.ai/v1"
    api_key: "${MISTRAL_API_KEY}"
```

**Utilisation dans une √©tape :**
```yaml
llm:
  enabled: true
  provider: "mistral_ai"
  model: "mistral-large-latest"  # ou "mistral-small-latest"
  temperature: 0.0
```

**Obtenir une cl√© :** https://console.mistral.ai/

## üîß Configuration par √âtape

### √âtape 4 : Enrichissement

**Cas d'usage :** Classification intelligente de documents

```yaml
# config/04_enrichment.yaml
llm:
  enabled: true  # Activer LLM pour classification
  provider: "ollama"
  model: "llama3"
  temperature: 0.0  # D√©terministe pour classification
  max_tokens: 500
```

**Avantages :** Classification plus pr√©cise qu'avec mots-cl√©s simples

### √âtape 5 : Audit

**Cas d'usage :** G√©n√©ration de r√©sum√©s narratifs d'audit

```yaml
# config/05_audit.yaml
llm:
  enabled: false  # D√©sactiv√© par d√©faut (logs structur√©s suffisent)
  provider: "ollama"
  model: "llama3"
  temperature: 0.3  # L√©g√®rement cr√©atif pour narratifs
  max_tokens: 1000
```

**Avantages :** R√©sum√©s lisibles pour rapports de conformit√©

### √âtape 3 : Chunking S√©mantique

**Cas d'usage :** D√©coupage bas√© sur similarit√© s√©mantique

```yaml
# config/03_chunking.yaml
strategy: "semantic"  # Activer chunking s√©mantique

semantic:
  provider: "openai"
  model: "text-embedding-3-large"
  similarity_threshold: 0.75
```

## üîê Gestion des Cl√©s API

### M√©thode 1 : Variables d'Environnement (Recommand√©)

```bash
# .env
export HUGGINGFACE_API_KEY="hf_xxxxx"
export MISTRAL_API_KEY="xxxxx"
export OPENAI_API_KEY="sk-xxxxx"
```

```yaml
# global.yaml
llm_providers:
  mistral_ai:
    api_key: "${MISTRAL_API_KEY}"  # Substitution automatique
```

### M√©thode 2 : Fichier de Secrets (Production)

```bash
# Cr√©er un fichier secrets.env (gitignored)
echo "MISTRAL_API_KEY=xxxxx" > secrets.env
source secrets.env
```

### ‚ö†Ô∏è M√©thode 3 : Hardcod√© (NON RECOMMAND√â)

```yaml
# NE JAMAIS FAIRE EN PRODUCTION
llm_providers:
  mistral_ai:
    api_key: "76cwpvjZqnFw1U0jLCEBKOHh5FprX2OJ"  # ‚ùå Visible dans git!
```

**Danger :** Les cl√©s commited dans git sont compromises imm√©diatement.

## üìä Exemples de Configuration Compl√®te

### Configuration D√©veloppement (Local)

```yaml
# config/global.yaml - Section llm_providers
llm_providers:
  ollama:
    access_method: "openai_compatible"
    base_url: "http://127.0.0.1:11434/v1"
    api_key: "ollama"

# config/04_enrichment.yaml
llm:
  enabled: true
  provider: "ollama"  # Gratuit, local
  model: "llama3"
  temperature: 0.0
```

### Configuration Production (Cloud)

```yaml
# config/global.yaml
llm_providers:
  mistral_ai:
    access_method: "openai_compatible"
    base_url: "https://api.mistral.ai/v1"
    api_key: "${MISTRAL_API_KEY}"

# config/04_enrichment.yaml
llm:
  enabled: true
  provider: "mistral_ai"  # API cloud professionnelle
  model: "mistral-large-latest"
  temperature: 0.0
```

## üéõÔ∏è Param√®tres Temp√©rature

| Temperature | Comportement | Cas d'usage |
|-------------|--------------|-------------|
| 0.0 | D√©terministe | Classification, extraction de donn√©es |
| 0.3 | L√©g√®rement vari√© | R√©sum√©s, narratifs d'audit |
| 0.7 | Cr√©atif | G√©n√©ration de rapports, suggestions |
| 1.0 | Tr√®s cr√©atif | Brainstorming (d√©conseill√© pour audit) |

**Recommandation :** Utiliser 0.0 pour toutes les t√¢ches d'audit et conformit√© (reproductibilit√©).

## üîÑ Migration depuis Ancienne Configuration

**Avant (global.yaml uniquement) :**
```yaml
llm_config:
  default_provider: "openai"
  openai:
    model: "gpt-4"
    temperature: 0.0
```

**Apr√®s (s√©paration infrastructure/fonctionnel) :**
```yaml
# global.yaml - Infrastructure
llm_providers:
  ollama:
    access_method: "openai_compatible"
    base_url: "http://127.0.0.1:11434/v1"
    api_key: "ollama"

# 04_enrichment.yaml - Fonctionnel
llm:
  enabled: true
  provider: "ollama"
  model: "llama3"
  temperature: 0.0
```

**Avantages :**
- ‚úÖ Chaque √©tape choisit son mod√®le optimal
- ‚úÖ Facile de tester diff√©rents providers
- ‚úÖ Configuration centralis√©e des connexions
- ‚úÖ Granularit√© fine (temp√©rature par t√¢che)

## üß™ Test de Configuration

```python
# test_llm_config.py
from rag_framework.config import load_config, load_step_config

# Charger config globale
global_config = load_config()
print("Providers disponibles:", global_config.llm_providers.keys())

# Charger config d'√©tape
step_config = load_step_config("04_enrichment.yaml")
if step_config.get("llm", {}).get("enabled"):
    print(f"LLM activ√©: {step_config['llm']['provider']}/{step_config['llm']['model']}")
```

## üìö Ressources

- [LM Studio](https://lmstudio.ai/) - Interface locale
- [Ollama](https://ollama.ai/) - Runner LLM open-source
- [vLLM](https://docs.vllm.ai/) - Serveur haute performance
- [Hugging Face](https://huggingface.co/) - Plateforme mod√®les
- [Mistral AI](https://mistral.ai/) - Provider fran√ßais

---

**Version:** 0.1.0
**Date:** 2025-10-30
**Statut:** ‚úÖ Configuration multi-provider op√©rationnelle
